{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Created while I was learning torch geometric and testing a few things\n",
    "# code (sort of) adapted from https://pytorch-geometric.readthedocs.io/en/latest/notes/introduction.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.datasets import Planetoid\n",
    "\n",
    "print(torch.__version__)\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 7\n",
      "Number of features: 1433\n",
      "Number of nodes: 2708\n",
      "Number of edges: 10556\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"data/\"\n",
    "cora = Planetoid(root=data_dir, name=\"Cora\")\n",
    "\n",
    "print(f\"Number of classes: {cora.num_classes}\")\n",
    "print(f\"Number of features: {cora.num_features}\")\n",
    "print(f\"Number of nodes: {cora.data.num_nodes}\")\n",
    "print(f\"Number of edges: {cora.data.num_edges}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(cora.num_node_features, 128)\n",
    "        self.conv2 = GCNConv(128, 32)\n",
    "        self.conv3 = GCNConv(32, cora.num_classes)\n",
    "        \n",
    "    def forward(self, cora):\n",
    "        x, edge_index = cora.x, cora.edge_index\n",
    "\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        #x = F.dropout(x, training=self.training)\n",
    "        x = self.conv3(x, edge_index)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.0001\n",
    "weight_decay = 5e-3\n",
    "epochs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 - loss: 1.9489637613296509\n",
      "epoch: 10 - loss: 1.9298394918441772\n",
      "epoch: 20 - loss: 1.9116084575653076\n",
      "epoch: 30 - loss: 1.8860547542572021\n",
      "epoch: 40 - loss: 1.8637586832046509\n",
      "epoch: 50 - loss: 1.831717610359192\n",
      "epoch: 60 - loss: 1.8016436100006104\n",
      "epoch: 70 - loss: 1.7624397277832031\n",
      "epoch: 80 - loss: 1.7200994491577148\n",
      "epoch: 90 - loss: 1.668863296508789\n",
      "epoch: 100 - loss: 1.6199707984924316\n",
      "epoch: 110 - loss: 1.5583577156066895\n",
      "epoch: 120 - loss: 1.509276032447815\n",
      "epoch: 130 - loss: 1.4529714584350586\n",
      "epoch: 140 - loss: 1.378476858139038\n",
      "epoch: 150 - loss: 1.3157808780670166\n",
      "epoch: 160 - loss: 1.2441827058792114\n",
      "epoch: 170 - loss: 1.1869970560073853\n",
      "epoch: 180 - loss: 1.1238960027694702\n",
      "epoch: 190 - loss: 1.0601407289505005\n",
      "epoch: 200 - loss: 1.0054330825805664\n",
      "epoch: 210 - loss: 0.945567786693573\n",
      "epoch: 220 - loss: 0.905239462852478\n",
      "epoch: 230 - loss: 0.8456275463104248\n",
      "epoch: 240 - loss: 0.8059667348861694\n",
      "epoch: 250 - loss: 0.7510144710540771\n",
      "epoch: 260 - loss: 0.7131742238998413\n",
      "epoch: 270 - loss: 0.6714447140693665\n",
      "epoch: 280 - loss: 0.6363585591316223\n",
      "epoch: 290 - loss: 0.6002485156059265\n",
      "epoch: 300 - loss: 0.5708916783332825\n",
      "epoch: 310 - loss: 0.5405916571617126\n",
      "epoch: 320 - loss: 0.5098492503166199\n",
      "epoch: 330 - loss: 0.49251189827919006\n",
      "epoch: 340 - loss: 0.4690459668636322\n",
      "epoch: 350 - loss: 0.4393852949142456\n",
      "epoch: 360 - loss: 0.4236283302307129\n",
      "epoch: 370 - loss: 0.3943713903427124\n",
      "epoch: 380 - loss: 0.38042518496513367\n",
      "epoch: 390 - loss: 0.3680829703807831\n",
      "epoch: 400 - loss: 0.35787659883499146\n",
      "epoch: 410 - loss: 0.3350290060043335\n",
      "epoch: 420 - loss: 0.31855708360671997\n",
      "epoch: 430 - loss: 0.2989649772644043\n",
      "epoch: 440 - loss: 0.28982508182525635\n",
      "epoch: 450 - loss: 0.2880595326423645\n",
      "epoch: 460 - loss: 0.2663949728012085\n",
      "epoch: 470 - loss: 0.26746079325675964\n",
      "epoch: 480 - loss: 0.2632177770137787\n",
      "epoch: 490 - loss: 0.2521510124206543\n",
      "epoch: 500 - loss: 0.24280032515525818\n",
      "epoch: 510 - loss: 0.24375583231449127\n",
      "epoch: 520 - loss: 0.22313399612903595\n",
      "epoch: 530 - loss: 0.2229079157114029\n",
      "epoch: 540 - loss: 0.20610079169273376\n",
      "epoch: 550 - loss: 0.20530517399311066\n",
      "epoch: 560 - loss: 0.20312738418579102\n",
      "epoch: 570 - loss: 0.21524326503276825\n",
      "epoch: 580 - loss: 0.1893938183784485\n",
      "epoch: 590 - loss: 0.19055721163749695\n",
      "epoch: 600 - loss: 0.1841733455657959\n",
      "epoch: 610 - loss: 0.1813337802886963\n",
      "epoch: 620 - loss: 0.18121393024921417\n",
      "epoch: 630 - loss: 0.17433440685272217\n",
      "epoch: 640 - loss: 0.16173115372657776\n",
      "epoch: 650 - loss: 0.16734479367733002\n",
      "epoch: 660 - loss: 0.16352148354053497\n",
      "epoch: 670 - loss: 0.14940233528614044\n",
      "epoch: 680 - loss: 0.15743908286094666\n",
      "epoch: 690 - loss: 0.1556749790906906\n",
      "epoch: 700 - loss: 0.14431500434875488\n",
      "epoch: 710 - loss: 0.14298762381076813\n",
      "epoch: 720 - loss: 0.14873702824115753\n",
      "epoch: 730 - loss: 0.1423749029636383\n",
      "epoch: 740 - loss: 0.13311044871807098\n",
      "epoch: 750 - loss: 0.13071349263191223\n",
      "epoch: 760 - loss: 0.13309486210346222\n",
      "epoch: 770 - loss: 0.13245412707328796\n",
      "epoch: 780 - loss: 0.13562434911727905\n",
      "epoch: 790 - loss: 0.13778744637966156\n",
      "epoch: 800 - loss: 0.12682975828647614\n",
      "epoch: 810 - loss: 0.1260083168745041\n",
      "epoch: 820 - loss: 0.12546183168888092\n",
      "epoch: 830 - loss: 0.12047341465950012\n",
      "epoch: 840 - loss: 0.118746317923069\n",
      "epoch: 850 - loss: 0.11308863759040833\n",
      "epoch: 860 - loss: 0.11769304424524307\n",
      "epoch: 870 - loss: 0.11759939044713974\n",
      "epoch: 880 - loss: 0.11310429126024246\n",
      "epoch: 890 - loss: 0.11594291776418686\n",
      "epoch: 900 - loss: 0.10954108834266663\n",
      "epoch: 910 - loss: 0.10393873602151871\n",
      "epoch: 920 - loss: 0.11494316160678864\n",
      "epoch: 930 - loss: 0.10752302408218384\n",
      "epoch: 940 - loss: 0.10561185330152512\n",
      "epoch: 950 - loss: 0.11019901931285858\n",
      "epoch: 960 - loss: 0.10030411928892136\n",
      "epoch: 970 - loss: 0.10442676395177841\n",
      "epoch: 980 - loss: 0.09748107939958572\n",
      "epoch: 990 - loss: 0.09770558029413223\n"
     ]
    }
   ],
   "source": [
    "model = GNN().to(device)\n",
    "data = cora.data.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"epoch: {epoch} - loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8100\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "pred = model(data).argmax(dim=1)\n",
    "correct = (pred[data.test_mask] == data.y[data.test_mask]).sum()\n",
    "acc = int(correct) / int(data.test_mask.sum())\n",
    "print('Accuracy: {:.4f}'.format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000])\n",
      "tensor([False,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True, False,  True,  True,  True,  True,\n",
      "         True,  True,  True, False,  True, False,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True, False,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True, False,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True, False,  True,  True,\n",
      "         True,  True,  True,  True, False, False,  True,  True,  True,  True,\n",
      "         True, False,  True, False, False, False, False,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True, False,  True,  True,  True,\n",
      "         True,  True, False,  True,  True,  True, False, False,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True, False,  True,  True, False,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True, False,  True, False,  True,  True,  True,  True,  True,  True,\n",
      "        False,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True, False, False,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True, False,  True,  True,\n",
      "        False,  True, False, False,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True, False,  True,  True,  True, False,  True,  True,  True,\n",
      "         True, False,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True, False,  True,  True, False,\n",
      "         True,  True,  True,  True,  True, False,  True,  True,  True,  True,\n",
      "         True, False, False,  True,  True,  True,  True,  True,  True,  True,\n",
      "        False, False, False,  True,  True, False,  True,  True,  True,  True,\n",
      "         True,  True, False,  True, False, False, False, False, False,  True,\n",
      "        False, False,  True,  True,  True, False,  True,  True,  True,  True,\n",
      "        False,  True,  True, False, False, False,  True, False,  True,  True,\n",
      "        False, False,  True, False, False,  True, False,  True,  True, False,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "        False,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True, False, False, False,  True,  True,  True,  True,  True,  True,\n",
      "        False,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True, False,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True, False, False, False, False,  True,  True,  True,\n",
      "        False,  True,  True, False,  True, False,  True,  True,  True,  True,\n",
      "         True, False,  True,  True,  True,  True, False, False,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True, False,  True, False, False,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True, False,  True,\n",
      "        False, False, False,  True,  True,  True,  True, False,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True, False,\n",
      "        False,  True,  True,  True,  True,  True,  True, False,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True, False,  True,\n",
      "         True,  True,  True,  True,  True, False,  True, False, False, False,\n",
      "         True,  True,  True,  True,  True, False,  True,  True,  True,  True,\n",
      "        False,  True,  True,  True,  True,  True, False,  True, False,  True,\n",
      "         True,  True,  True,  True, False,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True, False,  True, False,  True,  True,  True,  True,\n",
      "        False,  True,  True,  True, False,  True,  True,  True, False,  True,\n",
      "         True,  True,  True,  True,  True, False, False,  True, False,  True,\n",
      "         True, False, False,  True,  True, False,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True, False,  True,  True,  True,  True,  True,\n",
      "         True, False, False, False,  True,  True,  True, False, False,  True,\n",
      "        False, False,  True,  True,  True, False, False, False,  True, False,\n",
      "        False,  True, False,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True, False,  True,  True,  True,  True,\n",
      "         True,  True,  True, False,  True,  True,  True, False,  True,  True,\n",
      "        False,  True,  True,  True,  True,  True,  True,  True,  True, False,\n",
      "         True,  True,  True,  True, False,  True,  True, False, False,  True,\n",
      "         True, False,  True,  True,  True, False,  True,  True,  True,  True,\n",
      "        False,  True,  True,  True,  True, False,  True, False,  True, False,\n",
      "         True,  True,  True,  True,  True, False,  True,  True, False, False,\n",
      "        False,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True, False,  True,  True, False,  True,  True, False,  True,\n",
      "         True, False,  True,  True, False,  True,  True,  True,  True, False,\n",
      "        False, False, False,  True, False,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True, False,  True, False, False,  True,  True,\n",
      "         True,  True, False,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True, False, False,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True, False,  True,  True, False,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True, False,  True,  True, False,  True, False,\n",
      "         True, False, False, False, False,  True,  True,  True,  True,  True,\n",
      "        False,  True, False,  True,  True,  True, False,  True,  True,  True,\n",
      "         True, False, False,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True, False, False,  True,  True,  True,  True, False, False,\n",
      "         True, False,  True, False,  True,  True,  True,  True,  True, False,\n",
      "         True,  True, False,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True, False, False,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True, False,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True, False, False,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True, False,  True, False,  True,  True,  True, False,  True,\n",
      "         True,  True,  True,  True, False,  True, False, False,  True,  True],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "correct = (pred[data.test_mask] == data.y[data.test_mask])\n",
    "print(correct.shape)\n",
    "print(correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2708])"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.y.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
