{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn.conv import GCNConv, SAGEConv, GINConv, GATConv\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 7\n",
      "Number of features: 1433\n",
      "Number of nodes: 2708\n",
      "Number of edges: 10556\n",
      "Training nodes: 140\n",
      "Val nodes: 500\n",
      "Test nodes: 1000\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"~/tmp/data\"\n",
    "cora = Planetoid(root=data_dir, name=\"Cora\")\n",
    "\n",
    "print(f\"Number of classes: {cora.num_classes}\")\n",
    "print(f\"Number of features: {cora.num_features}\")\n",
    "print(f\"Number of nodes: {cora.data.num_nodes}\")\n",
    "print(f\"Number of edges: {cora.data.num_edges}\")\n",
    "print(f\"Training nodes: {cora.data.train_mask.sum()}\")\n",
    "print(f\"Val nodes: {cora.data.val_mask.sum()}\")\n",
    "print(f\"Test nodes: {cora.data.test_mask.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3)\n",
      "torch.Size([2708, 1433])\n",
      "tensor([ True,  True,  True,  ..., False, False, False])\n"
     ]
    }
   ],
   "source": [
    "print(cora.data.y[0])\n",
    "print(cora.data.x.shape)\n",
    "print(cora.data.train_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GCN, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = GCNConv(cora.num_features, 128)\n",
    "        self.conv2 = GCNConv(128, 32)\n",
    "        self.conv3 = GCNConv(32, cora.num_classes)\n",
    "                \n",
    "    def forward(self, h, edge_index):\n",
    "        h = self.conv1(h, edge_index)\n",
    "        h = h.relu()\n",
    "        h = F.dropout(h, p=0.5, training=self.training)\n",
    "        h = self.conv2(h, edge_index)\n",
    "        h = h.relu()\n",
    "        h = F.dropout(h, p=0.5, training=self.training)\n",
    "        h = self.conv3(h, edge_index)\n",
    "        \n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAGE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SAGE, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.layer1 = SAGEConv(cora.num_features, 128)\n",
    "        self.layer2 = SAGEConv(128, cora.num_classes)\n",
    "        \n",
    "    def forward(self, h, edge_index):\n",
    "        h = self.layer1(h, edge_index)\n",
    "        h = h.relu()\n",
    "        #h = F.dropout(h, p=0.5, training=self.training)\n",
    "        h = self.layer2(h, edge_index)\n",
    "        \n",
    "        return h        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GAT, self).__init__()\n",
    "        self.layer1 = GATConv(cora.num_features, 8, heads=8, dropout=0.6)\n",
    "        self.layer2 = GATConv(64, cora.num_classes, heads=1, dropout=0.6)\n",
    "    \n",
    "    def forward(self, h, edge_index):\n",
    "        h = self.layer1(h, edge_index)\n",
    "        h = F.elu(h)\n",
    "        h = self.layer2(h, edge_index)\n",
    "        \n",
    "        return F.softmax(h, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, epochs, epoch_log, criterion, optimizer, patience, monitor):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index)\n",
    "        loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            pred = out.argmax(dim=1)\n",
    "            correct_train = (pred[data.train_mask] == data.y[data.train_mask]).sum()\n",
    "            correct_val = (pred[data.val_mask] == data.y[data.val_mask]).sum()\n",
    "            train_acc = correct_train / data.train_mask.sum()\n",
    "            val_acc = correct_val / data.val_mask.sum()\n",
    "            val_loss = criterion(out[data.val_mask], data.y[data.val_mask])\n",
    "                    \n",
    "        if epoch % epoch_log == 0:\n",
    "            print(f\"epoch {epoch} - train_loss: {loss} - train_acc: {train_acc:0.4f} - val loss: {val_loss} - val_acc: {val_acc:0.4f}\")\n",
    "        \n",
    "        if \n",
    "        \n",
    "def test(model, data):\n",
    "    model.eval()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    pred = out.argmax(dim=1)\n",
    "    results = (pred[data.test_mask] == data.y[data.test_mask]).sum()\n",
    "    acc = results / data.test_mask.sum()\n",
    "    print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAT(\n",
      "  (layer1): GATConv(1433, 8, heads=8)\n",
      "  (layer2): GATConv(64, 7, heads=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "epochs = 1000\n",
    "epoch_log = 10\n",
    "weight_decay = 5e-4\n",
    "lr = 0.005\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "model = GAT().to(device)\n",
    "data = cora.data.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:0 to train.\n",
      "epoch 0 - train_loss: 1.9419984817504883 - train_acc: 0.1643 - val loss: 1.944545030593872 - val_acc: 0.1440\n",
      "epoch 10 - train_loss: 1.5857954025268555 - train_acc: 0.7643 - val loss: 1.7023471593856812 - val_acc: 0.6580\n",
      "epoch 20 - train_loss: 1.4634510278701782 - train_acc: 0.7929 - val loss: 1.5808606147766113 - val_acc: 0.6820\n",
      "epoch 30 - train_loss: 1.3651468753814697 - train_acc: 0.8357 - val loss: 1.5595567226409912 - val_acc: 0.6640\n",
      "epoch 40 - train_loss: 1.3078250885009766 - train_acc: 0.9071 - val loss: 1.5460660457611084 - val_acc: 0.6740\n",
      "epoch 50 - train_loss: 1.3330531120300293 - train_acc: 0.8643 - val loss: 1.5289453268051147 - val_acc: 0.6640\n",
      "epoch 60 - train_loss: 1.3466776609420776 - train_acc: 0.8500 - val loss: 1.5238271951675415 - val_acc: 0.6660\n",
      "epoch 70 - train_loss: 1.3376109600067139 - train_acc: 0.8714 - val loss: 1.545123815536499 - val_acc: 0.6560\n",
      "epoch 80 - train_loss: 1.3335446119308472 - train_acc: 0.8571 - val loss: 1.532637596130371 - val_acc: 0.6720\n",
      "epoch 90 - train_loss: 1.2723218202590942 - train_acc: 0.9286 - val loss: 1.5236225128173828 - val_acc: 0.6660\n",
      "epoch 100 - train_loss: 1.3207340240478516 - train_acc: 0.8929 - val loss: 1.5281227827072144 - val_acc: 0.6700\n",
      "epoch 110 - train_loss: 1.286142349243164 - train_acc: 0.8929 - val loss: 1.533097743988037 - val_acc: 0.6340\n",
      "epoch 120 - train_loss: 1.293690800666809 - train_acc: 0.8929 - val loss: 1.5390496253967285 - val_acc: 0.6420\n",
      "epoch 130 - train_loss: 1.2630512714385986 - train_acc: 0.9286 - val loss: 1.5099856853485107 - val_acc: 0.6680\n",
      "epoch 140 - train_loss: 1.313161849975586 - train_acc: 0.8786 - val loss: 1.5346004962921143 - val_acc: 0.6400\n",
      "epoch 150 - train_loss: 1.2848783731460571 - train_acc: 0.8857 - val loss: 1.5292716026306152 - val_acc: 0.6560\n",
      "epoch 160 - train_loss: 1.2921345233917236 - train_acc: 0.8786 - val loss: 1.5116578340530396 - val_acc: 0.6620\n",
      "epoch 170 - train_loss: 1.2924259901046753 - train_acc: 0.9071 - val loss: 1.4983469247817993 - val_acc: 0.6840\n",
      "epoch 180 - train_loss: 1.2704492807388306 - train_acc: 0.9214 - val loss: 1.5198570489883423 - val_acc: 0.6580\n",
      "epoch 190 - train_loss: 1.3229312896728516 - train_acc: 0.8429 - val loss: 1.5111485719680786 - val_acc: 0.6620\n",
      "epoch 200 - train_loss: 1.2995063066482544 - train_acc: 0.8643 - val loss: 1.5120168924331665 - val_acc: 0.6660\n",
      "epoch 210 - train_loss: 1.3255434036254883 - train_acc: 0.8429 - val loss: 1.5167765617370605 - val_acc: 0.6560\n",
      "epoch 220 - train_loss: 1.2920111417770386 - train_acc: 0.8857 - val loss: 1.5190387964248657 - val_acc: 0.6560\n",
      "epoch 230 - train_loss: 1.3190715312957764 - train_acc: 0.8571 - val loss: 1.5079154968261719 - val_acc: 0.6640\n",
      "epoch 240 - train_loss: 1.3210617303848267 - train_acc: 0.8714 - val loss: 1.5240333080291748 - val_acc: 0.6440\n",
      "epoch 250 - train_loss: 1.327478051185608 - train_acc: 0.8500 - val loss: 1.5097482204437256 - val_acc: 0.6660\n",
      "epoch 260 - train_loss: 1.2658069133758545 - train_acc: 0.9071 - val loss: 1.5136842727661133 - val_acc: 0.6500\n",
      "epoch 270 - train_loss: 1.3237669467926025 - train_acc: 0.8429 - val loss: 1.5134958028793335 - val_acc: 0.6580\n",
      "epoch 280 - train_loss: 1.3052476644515991 - train_acc: 0.8857 - val loss: 1.5205979347229004 - val_acc: 0.6600\n",
      "epoch 290 - train_loss: 1.2893040180206299 - train_acc: 0.8929 - val loss: 1.4973547458648682 - val_acc: 0.6720\n",
      "epoch 300 - train_loss: 1.2948225736618042 - train_acc: 0.8786 - val loss: 1.5237635374069214 - val_acc: 0.6260\n",
      "epoch 310 - train_loss: 1.2610796689987183 - train_acc: 0.9143 - val loss: 1.5320963859558105 - val_acc: 0.6440\n",
      "epoch 320 - train_loss: 1.332356572151184 - train_acc: 0.8429 - val loss: 1.5111486911773682 - val_acc: 0.6640\n",
      "epoch 330 - train_loss: 1.268959879875183 - train_acc: 0.9143 - val loss: 1.5092772245407104 - val_acc: 0.6520\n",
      "epoch 340 - train_loss: 1.3220250606536865 - train_acc: 0.8643 - val loss: 1.5189000368118286 - val_acc: 0.6420\n",
      "epoch 350 - train_loss: 1.3034093379974365 - train_acc: 0.8643 - val loss: 1.5199806690216064 - val_acc: 0.6540\n",
      "epoch 360 - train_loss: 1.308685541152954 - train_acc: 0.8571 - val loss: 1.5200332403182983 - val_acc: 0.6600\n",
      "epoch 370 - train_loss: 1.291393756866455 - train_acc: 0.8857 - val loss: 1.5276490449905396 - val_acc: 0.6320\n",
      "epoch 380 - train_loss: 1.3098680973052979 - train_acc: 0.8714 - val loss: 1.4869396686553955 - val_acc: 0.6840\n",
      "epoch 390 - train_loss: 1.368589997291565 - train_acc: 0.7929 - val loss: 1.5006351470947266 - val_acc: 0.6700\n",
      "epoch 400 - train_loss: 1.2876898050308228 - train_acc: 0.8929 - val loss: 1.4912227392196655 - val_acc: 0.6900\n",
      "epoch 410 - train_loss: 1.3099676370620728 - train_acc: 0.8571 - val loss: 1.4969501495361328 - val_acc: 0.6660\n",
      "epoch 420 - train_loss: 1.323233962059021 - train_acc: 0.8643 - val loss: 1.5104169845581055 - val_acc: 0.6520\n",
      "epoch 430 - train_loss: 1.2732923030853271 - train_acc: 0.9143 - val loss: 1.500449776649475 - val_acc: 0.6720\n",
      "epoch 440 - train_loss: 1.2968662977218628 - train_acc: 0.8714 - val loss: 1.541310429573059 - val_acc: 0.6120\n",
      "epoch 450 - train_loss: 1.295986533164978 - train_acc: 0.8714 - val loss: 1.5214393138885498 - val_acc: 0.6480\n",
      "epoch 460 - train_loss: 1.3061753511428833 - train_acc: 0.8643 - val loss: 1.513579249382019 - val_acc: 0.6560\n",
      "epoch 470 - train_loss: 1.2901874780654907 - train_acc: 0.8857 - val loss: 1.5133240222930908 - val_acc: 0.6560\n",
      "epoch 480 - train_loss: 1.297886610031128 - train_acc: 0.8857 - val loss: 1.5247151851654053 - val_acc: 0.6480\n",
      "epoch 490 - train_loss: 1.2705597877502441 - train_acc: 0.9000 - val loss: 1.4810844659805298 - val_acc: 0.6940\n",
      "epoch 500 - train_loss: 1.2769182920455933 - train_acc: 0.9000 - val loss: 1.5333895683288574 - val_acc: 0.6540\n",
      "epoch 510 - train_loss: 1.269704818725586 - train_acc: 0.9214 - val loss: 1.5219162702560425 - val_acc: 0.6480\n",
      "epoch 520 - train_loss: 1.2774795293807983 - train_acc: 0.9071 - val loss: 1.5117580890655518 - val_acc: 0.6620\n",
      "epoch 530 - train_loss: 1.2967485189437866 - train_acc: 0.8643 - val loss: 1.5243867635726929 - val_acc: 0.6380\n",
      "epoch 540 - train_loss: 1.2880905866622925 - train_acc: 0.8786 - val loss: 1.513588309288025 - val_acc: 0.6460\n",
      "epoch 550 - train_loss: 1.2907192707061768 - train_acc: 0.8929 - val loss: 1.4940916299819946 - val_acc: 0.6680\n",
      "epoch 560 - train_loss: 1.2809276580810547 - train_acc: 0.8929 - val loss: 1.4975389242172241 - val_acc: 0.6640\n",
      "epoch 570 - train_loss: 1.2809511423110962 - train_acc: 0.8929 - val loss: 1.503490686416626 - val_acc: 0.6680\n",
      "epoch 580 - train_loss: 1.2820720672607422 - train_acc: 0.8857 - val loss: 1.5313490629196167 - val_acc: 0.6140\n",
      "epoch 590 - train_loss: 1.2705146074295044 - train_acc: 0.9071 - val loss: 1.536175012588501 - val_acc: 0.6160\n",
      "epoch 600 - train_loss: 1.3208212852478027 - train_acc: 0.8643 - val loss: 1.5176283121109009 - val_acc: 0.6500\n",
      "epoch 610 - train_loss: 1.287415862083435 - train_acc: 0.8929 - val loss: 1.5166254043579102 - val_acc: 0.6360\n",
      "epoch 620 - train_loss: 1.3081907033920288 - train_acc: 0.8714 - val loss: 1.5233550071716309 - val_acc: 0.6320\n",
      "epoch 630 - train_loss: 1.3121602535247803 - train_acc: 0.8714 - val loss: 1.5204232931137085 - val_acc: 0.6360\n",
      "epoch 640 - train_loss: 1.3195884227752686 - train_acc: 0.8429 - val loss: 1.5258036851882935 - val_acc: 0.6480\n",
      "epoch 650 - train_loss: 1.2833787202835083 - train_acc: 0.8786 - val loss: 1.5264166593551636 - val_acc: 0.6440\n",
      "epoch 660 - train_loss: 1.304431676864624 - train_acc: 0.8714 - val loss: 1.521765947341919 - val_acc: 0.6440\n",
      "epoch 670 - train_loss: 1.2558022737503052 - train_acc: 0.9214 - val loss: 1.5097424983978271 - val_acc: 0.6680\n",
      "epoch 680 - train_loss: 1.2757014036178589 - train_acc: 0.8857 - val loss: 1.5260083675384521 - val_acc: 0.6300\n",
      "epoch 690 - train_loss: 1.2785861492156982 - train_acc: 0.9071 - val loss: 1.5248275995254517 - val_acc: 0.6240\n",
      "epoch 700 - train_loss: 1.2805713415145874 - train_acc: 0.8857 - val loss: 1.5224802494049072 - val_acc: 0.6360\n",
      "epoch 710 - train_loss: 1.2587255239486694 - train_acc: 0.9286 - val loss: 1.515535593032837 - val_acc: 0.6380\n",
      "epoch 720 - train_loss: 1.2925535440444946 - train_acc: 0.8714 - val loss: 1.5045942068099976 - val_acc: 0.6680\n",
      "epoch 730 - train_loss: 1.2952356338500977 - train_acc: 0.8857 - val loss: 1.5089194774627686 - val_acc: 0.6520\n",
      "epoch 740 - train_loss: 1.2976161241531372 - train_acc: 0.8643 - val loss: 1.503950595855713 - val_acc: 0.6520\n",
      "epoch 750 - train_loss: 1.273199439048767 - train_acc: 0.8786 - val loss: 1.5050907135009766 - val_acc: 0.6660\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 760 - train_loss: 1.303703784942627 - train_acc: 0.8714 - val loss: 1.522100806236267 - val_acc: 0.6520\n",
      "epoch 770 - train_loss: 1.3239892721176147 - train_acc: 0.8500 - val loss: 1.5233862400054932 - val_acc: 0.6340\n",
      "epoch 780 - train_loss: 1.265628457069397 - train_acc: 0.9071 - val loss: 1.519695520401001 - val_acc: 0.6380\n",
      "epoch 790 - train_loss: 1.290931224822998 - train_acc: 0.9000 - val loss: 1.5371614694595337 - val_acc: 0.6340\n",
      "epoch 800 - train_loss: 1.2920118570327759 - train_acc: 0.9071 - val loss: 1.518775224685669 - val_acc: 0.6540\n",
      "epoch 810 - train_loss: 1.2717015743255615 - train_acc: 0.8929 - val loss: 1.5351539850234985 - val_acc: 0.6300\n",
      "epoch 820 - train_loss: 1.2683625221252441 - train_acc: 0.9143 - val loss: 1.5338022708892822 - val_acc: 0.6100\n",
      "epoch 830 - train_loss: 1.2758690118789673 - train_acc: 0.8929 - val loss: 1.5189663171768188 - val_acc: 0.6400\n",
      "epoch 840 - train_loss: 1.3064175844192505 - train_acc: 0.8857 - val loss: 1.5215811729431152 - val_acc: 0.6320\n",
      "epoch 850 - train_loss: 1.306610107421875 - train_acc: 0.8643 - val loss: 1.5210981369018555 - val_acc: 0.6340\n",
      "epoch 860 - train_loss: 1.3153246641159058 - train_acc: 0.8429 - val loss: 1.5330942869186401 - val_acc: 0.6200\n",
      "epoch 870 - train_loss: 1.268195629119873 - train_acc: 0.9000 - val loss: 1.5184853076934814 - val_acc: 0.6420\n",
      "epoch 880 - train_loss: 1.287131905555725 - train_acc: 0.8857 - val loss: 1.5276470184326172 - val_acc: 0.6380\n",
      "epoch 890 - train_loss: 1.282539963722229 - train_acc: 0.8929 - val loss: 1.5434261560440063 - val_acc: 0.6100\n",
      "epoch 900 - train_loss: 1.3037925958633423 - train_acc: 0.8643 - val loss: 1.5397876501083374 - val_acc: 0.6220\n",
      "epoch 910 - train_loss: 1.2889354228973389 - train_acc: 0.8857 - val loss: 1.5349671840667725 - val_acc: 0.6280\n",
      "epoch 920 - train_loss: 1.290766716003418 - train_acc: 0.8714 - val loss: 1.5048832893371582 - val_acc: 0.6540\n",
      "epoch 930 - train_loss: 1.2813639640808105 - train_acc: 0.9000 - val loss: 1.5260432958602905 - val_acc: 0.6420\n",
      "epoch 940 - train_loss: 1.2586424350738525 - train_acc: 0.9071 - val loss: 1.5397499799728394 - val_acc: 0.6040\n",
      "epoch 950 - train_loss: 1.3107010126113892 - train_acc: 0.8429 - val loss: 1.5370641946792603 - val_acc: 0.6200\n",
      "epoch 960 - train_loss: 1.2981584072113037 - train_acc: 0.8929 - val loss: 1.5358914136886597 - val_acc: 0.6240\n",
      "epoch 970 - train_loss: 1.364847183227539 - train_acc: 0.8071 - val loss: 1.5213686227798462 - val_acc: 0.6380\n",
      "epoch 980 - train_loss: 1.313507318496704 - train_acc: 0.8786 - val loss: 1.5213232040405273 - val_acc: 0.6380\n",
      "epoch 990 - train_loss: 1.2983201742172241 - train_acc: 0.8714 - val loss: 1.511823296546936 - val_acc: 0.6500\n"
     ]
    }
   ],
   "source": [
    "print(f\"Using {device} to train.\")\n",
    "train(model, data, epochs, epoch_log, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8170, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "test(model, data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
